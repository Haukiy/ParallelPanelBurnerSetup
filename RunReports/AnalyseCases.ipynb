{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pylab\n",
    "import string\n",
    "import fdsreader\n",
    "import matplotlib\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from importlib import reload\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import spotpy\n",
    "from scipy.stats.qmc import LatinHypercube\n",
    "from pyDOE import lhs\n",
    "import random\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import plot_tree, export_graphviz\n",
    "import graphviz\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiment data and merg parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LHC Data\n",
    "#label_session=\"LHC_Session_2023-04-06_48\"\n",
    "#label_session=\"LHC_Session_2023-04-11_79\"\n",
    "#label_session=\"LHC_Session_2023-04-12_67\"\n",
    "#label_session=\"LHC_Session_2023-04-12_99\"\n",
    "label_session=\"LHC_Session_2023-04-13_94\"\n",
    "\n",
    "\n",
    "stor_id=label_session[12:]\n",
    "\n",
    "sims_path_seed = os.path.join(f\"../BurnerSims/{label_session}\")\n",
    "os.listdir(sims_path_seed)\n",
    "stor_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to experiment data\n",
    "exp_root = os.path.join(\"../GeneralInformation/macfp-db\")\n",
    "exp_macfp_dir = os.path.join(exp_root+\"/Fire_Growth/NIST_Parallel_Panel/Experimental_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the table of the centerline - Burner_HF_Centerline_multi-layer.csv\n",
    "centreline_hf_path = os.path.join(exp_macfp_dir, \"Burner_HF_Centerline_multi-layer.csv\")\n",
    "centreline_hf_df = pd.read_csv(centreline_hf_path, header=0, skiprows=[1])\n",
    "\n",
    "# Load the table ofthe centerline - Burner_steadyHF_Width_multi-layer.csv\n",
    "ST_Width_layer_path = os.path.join(exp_macfp_dir, \"Burner_steadyHF_Width_multi-layer.csv\")\n",
    "ST_Width_layer_df = pd.read_csv(ST_Width_layer_path, header=0, skiprows=[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data of the experiment Center - time and Width - steady state\n",
    "hf_TIME_center = centreline_hf_df[[\"HF_z20\", \"HF_z50\", \"HF_z75\", \"HF_z100\"]].to_numpy()\n",
    "hf_STEADY_width = ST_Width_layer_df[[\"HF_y-25\", \"HF_y-15\", \"HF_y0\", \"HF_y15\", \"HF_y25\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild ranges for default\n",
    "default_ranges = {}\n",
    "\n",
    "# Open the file for reading\n",
    "with open(f'{sims_path_seed}/param_set_ranges_{stor_id}.txt', 'r') as file:\n",
    "    \n",
    "    file.readline()\n",
    "\n",
    "   # Iterate through the lines of the file\n",
    "    for line in file:\n",
    "        \n",
    "        param_name, value1, value2, value3, value4 = [s.strip() for s in re.split(r':|,', line)]\n",
    "\n",
    "        # Convert the values from string to float\n",
    "        value1 = float(value1)\n",
    "        value2 = float(value2)\n",
    "        value3 = str(value3)\n",
    "        value4 = str(value4)\n",
    "\n",
    "        # Update the default_ranges dictionary\n",
    "        default_ranges[param_name] = (value1, value2)\n",
    "\n",
    "# Check \n",
    "print(\"Reconstructed dictionary:\")\n",
    "print(default_ranges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merg parameter values of each case to one dataframe\n",
    "TableData_path = \"TableData\"\n",
    "output_csv = f\"merged_param_values_{stor_id}.csv\"\n",
    "\n",
    "csv_files = [file for file in os.listdir(TableData_path) if file.startswith(label_session) and file.endswith(\"param_values.csv\")]\n",
    "\n",
    "merged_data = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    # Extract the case number from the filename\n",
    "    case_num = csv_file.split(\"_\")[0][4:]\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(os.path.join(TableData_path, csv_file))\n",
    "\n",
    "    # Delete the existing \"Case_sample\" column if it exists\n",
    "    if \"Case_sample\" in df.columns:\n",
    "        del df[\"Case_sample\"]\n",
    "        \n",
    "    # Add a new column for the case number and sample index\n",
    "    df.insert(0, \"Case_sample\", df.index.map(lambda x: f\"Case{case_num}_sample{x}\"))\n",
    "\n",
    "    # Append the dataframe to the merged_data list\n",
    "    merged_data.append(df)\n",
    "\n",
    "# Concatenate all the dataframes in the merged_data list\n",
    "merged_df = pd.concat(merged_data, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv(f'{TableData_path}/{output_csv}', index=False)\n",
    "LHC_sample_csv=pd.read_csv(f'{TableData_path}/{output_csv}', header=0)\n",
    "len(LHC_sample_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LHC_sample_csv[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Simulation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some necessary output\n",
    "devc=pd.read_csv(f\"P:/Shared/Studium/Masterthesis/ParallelPanelBurnerSetup/BurnerSims/LHC_Session_{stor_id}/case0/MaCFP_CASE0_sample0/MaCFP_Burner_CASE0_sample0_devc.csv\", header=0,skiprows=1)\n",
    "param_values = devc.loc[:, devc.columns.str.startswith(\"Time\")].values\n",
    "\n",
    "#condition\n",
    "search_entry=105\n",
    "indexes = np.where(param_values > search_entry)\n",
    "\n",
    "# first index that meets the condition\n",
    "first_index = indexes[0][0] if len(indexes[0]) > 0 else None\n",
    "\n",
    "# Simulation Time\n",
    "sim_time=125\n",
    "time_stepCSV=len(param_values)/sim_time\n",
    "\n",
    "\n",
    "print(\"Total length:\", len(param_values))\n",
    "print(\"Timestep:\", (time_stepCSV))\n",
    "print(f\"First index of entry higher than {search_entry}:\", first_index) # to average over the last 20 s\n",
    "print(f\"Tail:\", len(param_values)-first_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIM: Avg HF **CENTER** - Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DEVC IDs and desired times\n",
    "devc_ids = [\"HF_y0_z20\", \"HF_y0_z50\", \"HF_y0_z75\", \"HF_y0_z100\"]\n",
    "desired_times = [20, 40, 60, 80]  # s\n",
    "\n",
    "# Define time window to average over\n",
    "frame_window = 3\n",
    "\n",
    "# Create an empty dictionary to store the results\n",
    "SIM_avg_HF_Center_dict = {}\n",
    "\n",
    "# Loop through each case folder\n",
    "for case_folder in os.listdir(sims_path_seed):\n",
    "    if case_folder.startswith(\"case\"):\n",
    "        case_path = os.path.join(sims_path_seed, case_folder)\n",
    "        case_num = case_folder.split(\"case\")[-1]  # Extract the case number\n",
    "\n",
    "        # Loop through each subfolder and read the CSV file\n",
    "        for subdir in os.listdir(case_path):\n",
    "            if subdir.startswith(f\"MaCFP_CASE{case_num}_sample\"):\n",
    "                # Extract the sample number from the subfolder name\n",
    "                sample_num = subdir.split(\"_\")[-1]\n",
    "                #print(sample_num)\n",
    "\n",
    "                # Find the DEVC CSV file in the subfolder\n",
    "                for file in os.listdir(os.path.join(case_path, subdir)):\n",
    "                    if file.endswith(\"_devc.csv\") and file.startswith(f\"MaCFP_Burner_CASE{case_num}_sample\") and file.endswith(f\"{sample_num}_devc.csv\"):\n",
    "                        # Read the DEVC data from the CSV file\n",
    "                        devc_data = pd.read_csv(os.path.join(case_path, subdir, file), header=1)\n",
    "\n",
    "\n",
    "                        # Create an empty 2D array to store average flux values and MSEs\n",
    "                        SIM_avg_Fluxes_center = np.zeros((len(devc_ids) + 1, len(desired_times)))\n",
    "\n",
    "                        # Compute the average flux values for each DEVC ID and desired time\n",
    "                        for i, devc_id in enumerate(devc_ids):\n",
    "                            for j, desired_time in enumerate(desired_times):\n",
    "                                # Compute time window to average over\n",
    "                                frames = desired_time * 2\n",
    "                                t_min = int(frames - frame_window)\n",
    "                                t_max = int(frames + frame_window + 1)\n",
    "\n",
    "                                # Compute average within the time window\n",
    "                                flux_avrg = np.average(devc_data[devc_id][t_min:t_max].to_numpy())\n",
    "\n",
    "                                # Store the average flux value in the array\n",
    "                                SIM_avg_Fluxes_center[i, j] = flux_avrg\n",
    "\n",
    "                        # Compute the mean squared error (MSE) and the Root Mean Squared Error (RMSE) for each desired time\n",
    "                        for j, desired_time in enumerate(desired_times):\n",
    "                                MSE = np.mean((SIM_avg_Fluxes_center[:-1, j] - hf_TIME_center[j]) ** 2)\n",
    "                                SIM_avg_Fluxes_center[-1, j] = np.sqrt(MSE)\n",
    "                         \n",
    "                     #######   if int(sample_num[-1:]) < 10:\n",
    "                            sample_num = \"0\" + sample_num\n",
    "                        # Add the results to the dictionary using the sample number as the key\n",
    "                        SIM_avg_HF_Center_dict[f'Case{case_num}_{sample_num}'] = SIM_avg_Fluxes_center\n",
    "\n",
    "# Print the number of samples\n",
    "print(len(SIM_avg_HF_Center_dict))\n",
    "\n",
    "# Convert the  dictionary to a dataframe and save it to a CSV file\n",
    "data = {key: value.flatten() for key, value in SIM_avg_HF_Center_dict.items()}\n",
    "columns = []\n",
    "for j, desired_time in enumerate(desired_times):\n",
    "    for devc_id in devc_ids:\n",
    "        columns.append(f\"{devc_id}_T{desired_time}\")\n",
    "    columns.append(f\"RMSE_T{desired_time}\")\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient=\"index\", columns=columns)\n",
    "df.index.name = \"sample_num\"\n",
    "df.to_csv(f\"TableData/SIM_avg_HF_Center_{stor_id}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f\"TableData/SIM_avg_HF_Center_{stor_id}.csv\", header=0)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIM: **SURFACE** HF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DEVC IDs\n",
    "heights = [\"z20\", \"z50\", \"z75\", \"z100\"]\n",
    "y_positions = [\"-25\", \"-15\", \"0\", \"15\", \"25\"]\n",
    "devc_ids = [f\"HF_y{y_pos}_{height}\" for height in heights for y_pos in y_positions]\n",
    "\n",
    "SIM_HF_surface = {}\n",
    "\n",
    "# Loop through each case folder\n",
    "for case_folder in os.listdir(sims_path_seed):\n",
    "    if case_folder.startswith(\"case\"):\n",
    "        case_path = os.path.join(sims_path_seed, case_folder)\n",
    "        case_num = case_folder.split(\"case\")[-1]  # Extract the case number\n",
    "\n",
    "        # Loop through each subfolder and read the CSV file\n",
    "        for subdir in os.listdir(case_path):\n",
    "            if subdir.startswith(f\"MaCFP_CASE{case_num}_sample\"):\n",
    "                # Extract the sample number from the subfolder name\n",
    "                sample_num = subdir.split(\"_\")[-1]\n",
    "\n",
    "                # Find the DEVC CSV file in the subfolder\n",
    "                for file in os.listdir(os.path.join(case_path, subdir)):\n",
    "                    if file.endswith(\"_devc.csv\") and file.startswith(f\"MaCFP_Burner_CASE{case_num}_sample\") and file.endswith(f\"{sample_num}_devc.csv\"):\n",
    "                        # Read the DEVC data from the CSV file\n",
    "                        devc_data = pd.read_csv(os.path.join(case_path, subdir, file), header=1)\n",
    "                        \n",
    "                        # Compute the average for each devc_id over the last 20 s\n",
    "                        devc_data_avg = devc_data[devc_ids].tail(58).mean()\n",
    "\n",
    "                        # Reshape devc_data_avg to align with hf_STEADY_width\n",
    "                        devc_data_avg_reshaped = np.array(devc_data_avg).reshape(len(heights), len(y_positions))\n",
    "\n",
    "                        # Compute the mean squared error (MSE) and Root Mean Squared Error (RMSE) for the averaged data\n",
    "                        \n",
    "                        mse = ((devc_data_avg_reshaped - hf_STEADY_width) ** 2).mean(axis=1)\n",
    "                        rmse = np.sqrt(mse)\n",
    "                        \n",
    "                        # Add the results to the dictionary using the case and the sample number as the key\n",
    "                        SIM_HF_surface[f'Case{case_num}_{sample_num}'] = np.concatenate((devc_data_avg, rmse))\n",
    "                        \n",
    "# Print the number of samples\n",
    "print(len(SIM_HF_surface))\n",
    "# Convert the dictionary to a dataframe and save it to a CSV file\n",
    "columns = devc_ids + [f'RMSE_{height}' for height in heights]\n",
    "df = pd.DataFrame.from_dict(SIM_HF_surface, orient=\"index\", columns=columns)\n",
    "df.index.name = \"sample_num\"\n",
    "df.sort_index(inplace=True) \n",
    "df.to_csv(f\"TableData/SIM_HF_surface_{stor_id}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f\"TableData/SIM_HF_surface_{stor_id}.csv\", header=0)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **HRR** of all Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all HRR data to one file\n",
    "# Create an empty dictionary to store the HRR\n",
    "SIM_HRR = {}\n",
    "\n",
    "# Loop through each case folder\n",
    "for case_folder in os.listdir(sims_path_seed):\n",
    "    if case_folder.startswith(\"case\"):\n",
    "        case_path = os.path.join(sims_path_seed, case_folder)\n",
    "        # Extract the case number using regex\n",
    "        match = re.search(r'\\d+', case_folder)\n",
    "        if match:\n",
    "            case_num = match.group()\n",
    "            # Loop through each subfolder and read the CSV file\n",
    "            for subdir in os.listdir(case_path):\n",
    "                # Extract the sample number using regex\n",
    "                match = re.search(r'sample(\\d+)', subdir)\n",
    "                if match:\n",
    "                    sample_num = match.group(1)\n",
    "                    if subdir.startswith(f\"MaCFP_CASE{case_num}_sample{sample_num}\"):\n",
    "                        # Find the HRR CSV file in the subfolder\n",
    "                        for file in os.listdir(os.path.join(case_path, subdir)):\n",
    "                            if file.endswith(\"_hrr.csv\"):\n",
    "                                # Read the HRR data from the CSV file\n",
    "                                hrr_data = pd.read_csv(os.path.join(case_path, subdir, file), skiprows=[0], usecols=['HRR'])\n",
    "                                # Create a new row with the HRR data and the column name\n",
    "                                col_name = f\"CASE{case_num}__sample{sample_num}\"\n",
    "                                new_row = pd.DataFrame({col_name: hrr_data['HRR'].values.flatten()})\n",
    "                                SIM_HRR[f'Case{case_num}_{sample_num}'] = new_row\n",
    "\n",
    "# Convert the dictionary to a dataframe and save it to a CSV file\n",
    "SimHRR_data = pd.concat(SIM_HRR.values(), axis=1)                   \n",
    "# Save the HRR data to a new CSV file\n",
    "SimHRR_data.to_csv(f\"{TableData_path}/hrr_data_{stor_id}.csv\", index=False)\n",
    "HRR_Sims=pd.read_csv(f'{TableData_path}/hrr_data_{stor_id}.csv')\n",
    "\n",
    "print(len(HRR_Sims.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f\"TableData/hrr_data_{stor_id}.csv\", header=0)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HRR data\n",
    "HRR_Sims = pd.read_csv(f'{TableData_path}/hrr_data_{stor_id}.csv')\n",
    "\n",
    "# Plot all columns in one figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "# Plot each column\n",
    "for col in HRR_Sims.columns[1:]:\n",
    "    ax.plot(HRR_Sims[col], label=col)\n",
    "    ax.set_title(\"HRR for all cases and samples\")\n",
    "    ax.set_xlabel(\"Index\")\n",
    "    ax.set_ylabel(\"HRR\")\n",
    "    \n",
    "\n",
    "#plt.legend(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**avg. HRRPUA** of all Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store the average HRR values for each case\n",
    "HRRPUA_avg_lst = []\n",
    "# to calc the HRRPUA\n",
    "Burner_area=0.3*0.6\n",
    "# Loop through each column and calculate the average HRR over the last 58 entries\n",
    "for col in HRR_Sims.columns:\n",
    "    avg_hrr = HRR_Sims[col].tail(58).mean()\n",
    "    HRRPUA_avg_lst.append(avg_hrr/Burner_area)\n",
    "    \n",
    "HRRPUA_avg=np.array([HRRPUA_avg_lst])\n",
    "print((HRRPUA_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LHC Parameter** overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LHC Parameters for an overview\n",
    "# Create a grid of plots with 2 columns\n",
    "fig, axs = plt.subplots(nrows=len(LHC_sample_csv.columns), ncols=2, figsize=(12, 24))\n",
    "\n",
    "# Analyze distribution and plot line for each column\n",
    "for i, col in enumerate(LHC_sample_csv.columns[1:]):\n",
    "    axs[i, 0].hist(LHC_sample_csv[col],bins=10)\n",
    "    axs[i, 0].set_title(f\"Distribution of {col}\")\n",
    "    axs[i, 0].set_xlabel(col)\n",
    "    axs[i, 0].set_ylabel(\"Frequency\")\n",
    "    axs[i, 1].plot(LHC_sample_csv[col])\n",
    "    axs[i, 1].set_title(col)\n",
    "    axs[i, 1].set_xlabel(\"Index\")\n",
    "    axs[i, 1].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Impact of the Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression\n",
    "\n",
    "Random Forest regression is an ensemble learning method that constructs multiple decision trees and combines their results to improve the overall predictive performance of the model. The method works well for both regression and classification tasks. In this case, we're using a Random Forest regression model to analyze the relationship between the input parameters and the RMSE values [[Towards Data Science](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)].\n",
    "\n",
    "The diagram below illustrates the basic structure of a Random Forest model:\n",
    "\n",
    "![Random Forest](https://miro.medium.com/v2/resize:fit:640/format:webp/1*LMoJmXCsQlciGTEyoSN39g.jpeg)\n",
    "\n",
    "*Image Source: [Towards Data Science](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)*\n",
    "\n",
    "1. The Random Forest algorithm constructs multiple decision trees by using a random selection of features and data points.\n",
    "2. Each decision tree in the ensemble produces its own output (in this case, an RMSE value).\n",
    "3. The final output of the Random Forest model is obtained by averaging the outputs of all the decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: a single decision tree from the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names for parameter data and simulation data\n",
    "lhc_sess_param = [file for file in os.listdir(TableData_path) if file.startswith(\"merged_param_values\") and file.endswith(\".csv\")]\n",
    "sim_hf_sess = [file for file in os.listdir(TableData_path) if file.startswith(\"SIM_HF_surface\") and file.endswith(\".csv\")]\n",
    "\n",
    "all_rmse_values = []\n",
    "all_lhc_dfs = []\n",
    "\n",
    "\n",
    "# Loop through each \"package\" of simulations\n",
    "for i in range(len(lhc_sess_param)):\n",
    "    \n",
    "    lhc_df = pd.read_csv(f'{TableData_path}/{lhc_sess_param[i]}', header=0)\n",
    "\n",
    "    # Read the simulation data\n",
    "    sim_hf_df = pd.read_csv(f'{TableData_path}/{sim_hf_sess[i]}', header=0)\n",
    "\n",
    "    # Get the RMSE values\n",
    "    rmse_values = sim_hf_df.loc[:, sim_hf_df.columns.str.startswith(\"RMSE\")].values\n",
    "    rmse_average = rmse_values.mean(axis=1)\n",
    "\n",
    "    all_rmse_values.append(rmse_average)\n",
    "    all_lhc_dfs.append(lhc_df)\n",
    "\n",
    "# Combine the data from all \"packages\"\n",
    "combined_rmse_values = np.concatenate(all_rmse_values)\n",
    "combined_lhc_df = pd.concat(all_lhc_dfs)\n",
    "\n",
    "# Get the unique parameter names\n",
    "unique_params = [col for col in combined_lhc_df.columns if col != 'Case_sample']\n",
    "\n",
    "# Get the parameter data\n",
    "X = combined_lhc_df[unique_params].values\n",
    "\n",
    "# Fit the Random Forest model\n",
    "regr = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "regr.fit(X, combined_rmse_values)\n",
    "\n",
    "# Visualize a single decision tree from the Random Forest model\n",
    "\n",
    "def visualize_tree(tree, feature_names, max_depth):\n",
    "    dot_data = export_graphviz(tree, out_file=None, \n",
    "                               feature_names=feature_names,  \n",
    "                               filled=True, rounded=True,  \n",
    "                               special_characters=True,\n",
    "                               max_depth=max_depth)  \n",
    "    graph = graphviz.Source(dot_data) \n",
    "    graph.format = 'png' # Set the output format to PNG\n",
    "    image_data = graph.pipe(format='png') # Get the binary image data as bytes\n",
    "    image = Image.open(BytesIO(image_data)) # Create an Image object from the binary data\n",
    "    return image\n",
    "\n",
    "# Get the first decision tree from the Random Forest model\n",
    "first_tree = regr.estimators_[0]\n",
    "\n",
    "# Visualize the first decision tree with a maximum depth of 3\n",
    "tree_image = visualize_tree(first_tree, unique_params, max_depth=4)\n",
    "display(tree_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surface Anlaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names for parameter data and simulation data\n",
    "lhc_sess_param = [file for file in os.listdir(TableData_path) if file.startswith(\"merged_param_values\") and file.endswith(\".csv\")]\n",
    "sim_hf_sess = [file for file in os.listdir(TableData_path) if file.startswith(\"SIM_HF_surface\") and file.endswith(\".csv\")]\n",
    "\n",
    "all_rmse_values = []\n",
    "all_lhc_dfs = []\n",
    "\n",
    "# Loop through each \"package\" of simulations\n",
    "for i in range(len(lhc_sess_param)):\n",
    "    # Read the parameter data\n",
    "    lhc_df = pd.read_csv(f'{TableData_path}/{lhc_sess_param[i]}', header=0)\n",
    "\n",
    "    # Read the simulation data\n",
    "    sim_hf_df = pd.read_csv(f'{TableData_path}/{sim_hf_sess[i]}', header=0)\n",
    "\n",
    "    # Get the RMSE values\n",
    "    rmse_values = sim_hf_df.loc[:, sim_hf_df.columns.str.startswith(\"RMSE\")].values\n",
    "    rmse_average = rmse_values.mean(axis=1)\n",
    "\n",
    "    all_rmse_values.append(rmse_average)\n",
    "    all_lhc_dfs.append(lhc_df)\n",
    "\n",
    "# Combine the data from all \"packages\"\n",
    "combined_rmse_values = np.concatenate(all_rmse_values)\n",
    "combined_lhc_df = pd.concat(all_lhc_dfs)\n",
    "\n",
    "# Remove the 'Case_sample' column from the combined_lhc_df\n",
    "combined_lhc_df = combined_lhc_df.drop(columns=['Case_sample'])\n",
    "\n",
    "# Get the unique parameter names from the combined_lhc_df\n",
    "unique_params = combined_lhc_df.columns.tolist()\n",
    "\n",
    "# Create a Random Forest regression model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model using the combined parameter sets and the combined RMSE values\n",
    "X = combined_lhc_df[unique_params].values\n",
    "y = combined_rmse_values\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Calculate the feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Combine the parameter names and their importances into a DataFrame\n",
    "importances_df = pd.DataFrame({'Parameter': unique_params, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame based on the importances\n",
    "sorted_importances_df = importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the sorted importances DataFrame\n",
    "print(sorted_importances_df)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(22, 6))\n",
    "plt.bar(sorted_importances_df['Parameter'], sorted_importances_df['Importance'])\n",
    "plt.xlabel('Parameters')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Parameter Importance - Surface Anlaysis -')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file names for parameter data and simulation data\n",
    "lhc_sess_param = [file for file in os.listdir(TableData_path) if file.startswith(\"merged_param_values\") and file.endswith(\".csv\")]\n",
    "sim_hf_sess_center = [file for file in os.listdir(TableData_path) if file.startswith(\"SIM_avg_HF_Center\") and file.endswith(\".csv\")]\n",
    "\n",
    "all_rmse_values = []\n",
    "all_lhc_dfs = []\n",
    "\n",
    "# Loop through each \"package\" of simulations\n",
    "for i in range(len(lhc_sess_param)):\n",
    "    # Read the parameter data\n",
    "    lhc_df = pd.read_csv(f'{TableData_path}/{lhc_sess_param[i]}', header=0)\n",
    "\n",
    "    # Read the simulation data\n",
    "    sim_hf_df = pd.read_csv(f'{TableData_path}/{sim_hf_sess_center[i]}', header=0)\n",
    "\n",
    "    # Get the RMSE values\n",
    "    rmse_values = sim_hf_df.loc[:, sim_hf_df.columns.str.startswith(\"RMSE\")].values\n",
    "    rmse_average = rmse_values.mean(axis=1)\n",
    "\n",
    "    all_rmse_values.append(rmse_average)\n",
    "    all_lhc_dfs.append(lhc_df)\n",
    "\n",
    "# Combine the data from all \"packages\"\n",
    "combined_rmse_values = np.concatenate(all_rmse_values)\n",
    "combined_lhc_df = pd.concat(all_lhc_dfs)\n",
    "\n",
    "# Remove the 'Case_sample' column from the combined_lhc_df\n",
    "combined_lhc_df = combined_lhc_df.drop(columns=['Case_sample'])\n",
    "\n",
    "# Get the unique parameter names from the combined_lhc_df\n",
    "unique_params = combined_lhc_df.columns.tolist()\n",
    "\n",
    "# Create a Random Forest regression model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model using the combined parameter sets and the combined RMSE values\n",
    "X = combined_lhc_df[unique_params].values\n",
    "y = combined_rmse_values\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Calculate the feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Combine the parameter names and their importances into a DataFrame\n",
    "importances_df = pd.DataFrame({'Parameter': unique_params, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame based on the importances\n",
    "sorted_importances_df = importances_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the sorted importances DataFrame\n",
    "print(sorted_importances_df)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(22, 6))\n",
    "plt.bar(sorted_importances_df['Parameter'], sorted_importances_df['Importance'])\n",
    "plt.xlabel('Parameters')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Parameter Importance - Time Analysis -')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Surface** anlaysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see `Burner_steadyHF_Width_multi-layer.csv` of the [MaCFP/NIST parallel panel tests](https://github.com/MaCFP/macfp-db/tree/master/Fire_Growth/NIST_Parallel_Panel/Documentation).\n",
    "\n",
    "Timsteps: steady state for device: HF_y-25, HF_y-15, HF_y0, HF_y15, HF_y25 with height: 20, 50, 75, 100\n",
    "\n",
    "**Experiment Plot**\n",
    "\n",
    "<img src=\"AssessBurnerOutput/BurnerPanelFluxMapExp.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_hf_df = pd.read_csv(f'{TableData_path}/SIM_HF_surface_{stor_id}.csv', header=0)\n",
    "# Calculate the average RMSE for each iteration\n",
    "rmse_values = sim_hf_df.loc[:, sim_hf_df.columns.str.startswith(\"RMSE\")].values\n",
    "rmse_average = rmse_values.mean(axis=1)\n",
    "\n",
    "# Create a 1x2 grid of subplots for the line and bar plots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "# Line plot of the average RMSE data\n",
    "axes[0].plot(rmse_average, marker='o', linestyle='-', linewidth=2)\n",
    "axes[0].set_xlabel('Samples')\n",
    "axes[0].set_ylabel('Average RMSE')\n",
    "axes[0].set_title('Average RMSE over Samples (Line Plot)')\n",
    "axes[0].grid(True,alpha=0.5)\n",
    "\n",
    "# Bar plot of the average RMSE data\n",
    "axes[1].bar(range(len(rmse_average)), rmse_average)\n",
    "axes[1].set_xlabel('Samples')\n",
    "axes[1].set_ylabel('Average RMSE')\n",
    "axes[1].set_title('Average RMSE over Samples (Bar Plot)')\n",
    "axes[1].grid(True,alpha=0.5)\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation avg. HRRPUA and avg. RMSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_hf_df = pd.read_csv(f'{TableData_path}/SIM_HF_surface_{stor_id}.csv', header=0)\n",
    "\n",
    "# Extract the RMSE values and HRRPUA_avg\n",
    "rmse_values = sim_hf_df.loc[:, sim_hf_df.columns.str.startswith(\"RMSE\")].values\n",
    "rmse_average = rmse_values.mean(axis=1)\n",
    "HRRPUA_avg = HRRPUA_avg.ravel()\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.scatter(HRRPUA_avg, rmse_average)\n",
    "\n",
    "# Add a trend line to the plot\n",
    "z = np.polyfit(HRRPUA_avg, rmse_average, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(HRRPUA_avg,p(HRRPUA_avg),\"r--\")\n",
    "\n",
    "# Add axis labels and a title\n",
    "plt.xlabel('avg. HRRPUA')\n",
    "plt.ylabel('avg. RMSE')\n",
    "plt.title('Surface (width): Correlation between HRRPUA_avg and RMSE Average')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HRRPUA vs. Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files\n",
    "lhc_df = pd.read_csv(f'{TableData_path}/{output_csv}', header=0)\n",
    "sim_hf_df = pd.read_csv(f'{TableData_path}/SIM_HF_surface_{stor_id}.csv', header=0)\n",
    "\n",
    "# Extract the parameter values, HRRPUA_avg, and RMSE values\n",
    "param = \"TMPA\"\n",
    "rmse_values = sim_hf_df.loc[:, sim_hf_df.columns.str.startswith(\"RMSE\")].values\n",
    "rmse_average = rmse_values.mean(axis=1)\n",
    "param_values = lhc_df.loc[:, lhc_df.columns.str.startswith(param)].values.ravel()\n",
    "HRRPUA_avg = HRRPUA_avg.ravel()\n",
    "\n",
    "# Get the first column of lhc_df as the labels\n",
    "labels = lhc_df.iloc[:, 0].values\n",
    "\n",
    "# Combine the labels, HRRPUA_avg, param_values, and rmse_average\n",
    "data_dict = {\n",
    "    i: {\n",
    "        \"case_id\": lhc_df.iloc[i, 0],\n",
    "        \"HRRPUA_avg\": HRRPUA_avg[i],\n",
    "        \"param_value\": param_values[i],\n",
    "        \"rmse_average\": rmse_average[i],\n",
    "    }\n",
    "    for i in range(len(HRRPUA_avg))\n",
    "}\n",
    "\n",
    "\n",
    "# Calculate the min and max values of the colorbar based on the rmse_average values\n",
    "rmse_range = np.max(rmse_average) - np.min(rmse_average)\n",
    "cbar_min = np.min(rmse_average) - 0.1 * rmse_range\n",
    "cbar_max = np.max(rmse_average) + 0.1 * rmse_range\n",
    "\n",
    "# Create a colormap for the rmse_average range\n",
    "norm = plt.Normalize(vmin=cbar_min, vmax=cbar_max)\n",
    "cmap = plt.get_cmap(\"jet\")\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.set_xlabel(\"avg. HRRPUA\")\n",
    "ax.set_ylabel(param)\n",
    "ax.set_title(f\"Surface (width): avg. HRRPUA vs {param}\")\n",
    "\n",
    "# Plot each entry in the data_dict as a scatter plot point\n",
    "for label, data in data_dict.items():\n",
    "    hrrpua_avg = data[\"HRRPUA_avg\"]\n",
    "    param_value = data[\"param_value\"]\n",
    "    rmse_average = data[\"rmse_average\"]\n",
    "    color = cmap(norm(rmse_average))\n",
    "    ax.scatter(hrrpua_avg, param_value, c=[color])\n",
    "\n",
    "# Add a colorbar to represent the rmse_average range\n",
    "cbar = plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap))\n",
    "cbar.set_label('avg. RMSE')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Time** Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see `Burner_HF_Centerline_multi-layer.csv` of the [MaCFP/NIST parallel panel tests](https://github.com/MaCFP/macfp-db/tree/master/Fire_Growth/NIST_Parallel_Panel/Documentation).\n",
    "\n",
    "Timsteps: 20,40,60,80 for device: HF_z20, HF_z50, HF_z75, HF_z100\n",
    "\n",
    "**Experiment Plot**\n",
    "\n",
    "<img src=\"AssessBurnerOutput/BurnerPanelCentreLineHeatFluxExp.png\" width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_hf_df_Center = pd.read_csv(f\"TableData/SIM_avg_HF_Center_{stor_id}.csv\", header=0)\n",
    "# Calculate the average RMSE for each iteration\n",
    "rmse_values = sim_hf_df_Center.loc[:, sim_hf_df_Center.columns.str.startswith(\"RMSE\")].values\n",
    "rmse_average = rmse_values.mean(axis=1)\n",
    "\n",
    "# Create a 1x2 grid of subplots for the line and bar plots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "# Line plot of the average RMSE data\n",
    "axes[0].plot(rmse_average, marker='o', linestyle='-', linewidth=2)\n",
    "axes[0].set_xlabel('Samples')\n",
    "axes[0].set_ylabel('Average RMSE')\n",
    "axes[0].set_title('Average RMSE over Samples (Line Plot)')\n",
    "axes[0].grid(True,alpha=0.5)\n",
    "\n",
    "# Bar plot of the average RMSE data\n",
    "axes[1].bar(range(len(rmse_average)), rmse_average)\n",
    "axes[1].set_xlabel('Samples')\n",
    "axes[1].set_ylabel('Average RMSE')\n",
    "axes[1].set_title('Average RMSE over Samples (Bar Plot)')\n",
    "axes[1].grid(True,alpha=0.5)\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation avg. HRRPUA and avg. RMSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_hf_df_Center = pd.read_csv(f\"TableData/SIM_avg_HF_Center_{stor_id}.csv\", header=0)\n",
    "\n",
    "# Extract the RMSE values and HRRPUA_avg\n",
    "rmse_values = sim_hf_df_Center.loc[:, sim_hf_df_Center.columns.str.startswith(\"RMSE\")].values\n",
    "rmse_average = rmse_values.mean(axis=1)\n",
    "HRRPUA_avg = HRRPUA_avg.ravel()\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.scatter(HRRPUA_avg, rmse_average)\n",
    "\n",
    "# Add a trend line to the plot\n",
    "z = np.polyfit(HRRPUA_avg, rmse_average, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(HRRPUA_avg,p(HRRPUA_avg),\"r--\")\n",
    "\n",
    "# Add axis labels and a title\n",
    "plt.xlabel('avg. HRRPUA')\n",
    "plt.ylabel('avg. RMSE')\n",
    "plt.title('Center (Time): Correlation between HRRPUA_avg and RMSE Average')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HRRPUA vs. Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files\n",
    "lhc_df = pd.read_csv(f'{TableData_path}/{output_csv}', header=0)\n",
    "sim_hf_df_Center = pd.read_csv(f\"TableData/SIM_avg_HF_Center_{stor_id}.csv\", header=0)\n",
    "\n",
    "# Extract the parameter values, HRRPUA_avg, and RMSE values\n",
    "param = \"TMPA\"\n",
    "rmse_values = sim_hf_df_Center.loc[:, sim_hf_df_Center.columns.str.startswith(\"RMSE\")].values\n",
    "rmse_average = rmse_values.mean(axis=1)\n",
    "param_values = lhc_df.loc[:, lhc_df.columns.str.startswith(param)].values.ravel()\n",
    "HRRPUA_avg = HRRPUA_avg.ravel()\n",
    "\n",
    "# Get the first column of lhc_df as the labels\n",
    "labels = lhc_df.iloc[:, 0].values\n",
    "\n",
    "# Combine the labels, HRRPUA_avg, param_values, and rmse_average\n",
    "data_dict = {\n",
    "    i: {\n",
    "        \"case_id\": lhc_df.iloc[i, 0],\n",
    "        \"HRRPUA_avg\": HRRPUA_avg[i],\n",
    "        \"param_value\": param_values[i],\n",
    "        \"rmse_average\": rmse_average[i],\n",
    "    }\n",
    "    for i in range(len(HRRPUA_avg))\n",
    "}\n",
    "\n",
    "\n",
    "# Calculate the min and max values of the colorbar based on the rmse_average values\n",
    "rmse_range = np.max(rmse_average) - np.min(rmse_average)\n",
    "cbar_min = np.min(rmse_average) - 0.1 * rmse_range\n",
    "cbar_max = np.max(rmse_average) + 0.1 * rmse_range\n",
    "\n",
    "# Create a colormap for the rmse_average range\n",
    "norm = plt.Normalize(vmin=cbar_min, vmax=cbar_max)\n",
    "cmap = plt.get_cmap(\"jet\")\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.set_xlabel(\"avg. HRRPUA\")\n",
    "ax.set_ylabel(param)\n",
    "ax.set_title(f\"Center (Time): avg. HRRPUA vs {param}\")\n",
    "\n",
    "# Plot each entry in the data_dict as a scatter plot point\n",
    "for label, data in data_dict.items():\n",
    "    hrrpua_avg = data[\"HRRPUA_avg\"]\n",
    "    param_value = data[\"param_value\"]\n",
    "    rmse_average = data[\"rmse_average\"]\n",
    "    color = cmap(norm(rmse_average))\n",
    "    ax.scatter(hrrpua_avg, param_value, c=[color])\n",
    "\n",
    "# Add colorbar \n",
    "cbar = plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap))\n",
    "cbar.set_label('avg. RMSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Range: pymoo - Multi-objective Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Ref.:</b> J. Blank and K. Deb, \"Pymoo: Multi-Objective Optimization in Python,\" in IEEE Access, vol. 8, pp. 89497-89509, 2020, doi: <a href=\"https://ieeexplore.ieee.org/document/9078759\">10.1109./ACCESS.2020.2990567</a>.\n",
    "</div>\n",
    "\n",
    "The **Genetic Algorithm (GA)** is a optimization technique inspired by the process of natural selection and evolution. It works by iteratively evolving a population of candidate solutions towards an optimal solution. The GA has several key components, including selection, crossover, and mutation, which work together to explore and exploit the solution space effectively.\n",
    "<a href=\"https://pymoo.org/algorithms/soo/ga.html\">[1]</a>\n",
    "\n",
    "\n",
    "* 1. Initialization: Generate an initial population of candidate solutions randomly or using a sampling method. Each solution is called an individual and is represented as a chromosome, which encodes the decision variables.\n",
    "\n",
    "* 2. Evaluation: Calculate the fitness value for each individual in the population using the objective function(s). In single-objective optimization, the fitness value is often the same as the objective value.\n",
    "\n",
    "* 3. Selection: Select a subset of individuals from the population to act as parents for the next generation. Selection is usually based on fitness values, with better-performing individuals having a higher probability of being chosen. This process is known as fitness-proportional or roulette-wheel selection. Other selection methods include tournament selection, rank-based selection, and truncation selection.\n",
    "\n",
    "* 4. Crossover: Apply the crossover (also called recombination) operator to pairs of selected parents to generate offspring. Crossover combines the genetic material from two parents to create one or more offspring, which inherit features from both parents. There are various crossover techniques, such as one-point, two-point, or uniform crossover, depending on the representation of the chromosomes.\n",
    "\n",
    "* 5. Mutation: Apply the mutation operator to the offspring with a certain probability. Mutation introduces small random changes to the offspring's genetic material, promoting diversity in the population and helping the search algorithm explore the solution space more effectively. Mutation operators depend on the representation of the chromosomes and include bit-flip mutation for binary strings, swap mutation for permutations, and Gaussian mutation for real-valued decision variables.\n",
    "\n",
    "* 6. Replacement: Replace some or all individuals in the current population with the newly generated offspring. Replacement strategies include generational replacement, where the entire population is replaced, and steady-state replacement, where only a portion of the population is replaced.\n",
    "\n",
    "* 7. Termination: Check if a termination criterion has been met, such as reaching a maximum number of generations, achieving a desired fitness value, or not observing significant improvements in the best fitness value for a certain number of generations. If the termination criterion is not met, return to step 2 (evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data for demonstration purposes\n",
    "population = np.random.rand(50, 2)\n",
    "fitness_values = np.random.rand(50)\n",
    "selection_counts = np.random.randint(1, 10, 50)\n",
    "parent1, parent2 = np.random.rand(10), np.random.rand(10)\n",
    "offspring_before_mutation, offspring_after_mutation = np.random.rand(10), np.random.rand(10)\n",
    "population_next_gen = np.random.rand(50, 2)\n",
    "best_fitness = np.random.rand(50)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
    "\n",
    "# 1. Initialization\n",
    "axes[0, 0].scatter(population[:, 0], population[:, 1])\n",
    "axes[0, 0].set_title(\"1. Initialization\")\n",
    "axes[0, 0].set_xlabel(\"Variable 1\")\n",
    "axes[0, 0].set_ylabel(\"Variable 2\")\n",
    "\n",
    "# 2. Evaluation\n",
    "axes[0, 1].plot(fitness_values, marker=\"o\", linestyle=\"\")\n",
    "axes[0, 1].set_title(\"2. Evaluation\")\n",
    "axes[0, 1].set_xlabel(\"Individual Index\")\n",
    "axes[0, 1].set_ylabel(\"Fitness Value\")\n",
    "\n",
    "# 3. Selection\n",
    "axes[0, 2].bar(np.arange(len(selection_counts)), selection_counts)\n",
    "axes[0, 2].set_title(\"3. Selection\")\n",
    "axes[0, 2].set_xlabel(\"Individual Index\")\n",
    "axes[0, 2].set_ylabel(\"Selection Count\")\n",
    "\n",
    "# 4. Crossover\n",
    "parent1_plot, = axes[0, 3].plot(parent1, color=\"blue\", label=\"Parent 1\")\n",
    "parent2_plot, = axes[0, 3].plot(parent2, color=\"red\", label=\"Parent 2\")\n",
    "axes[0, 3].set_title(\"4. Crossover\")\n",
    "axes[0, 3].set_xlabel(\"Chromosome Index\")\n",
    "axes[0, 3].set_ylabel(\"Decision Variable\")\n",
    "axes[0, 3].legend()\n",
    "\n",
    "# 5. Mutation\n",
    "before_mutation_plot, = axes[1, 0].plot(offspring_before_mutation, linestyle=\"--\", color=\"blue\", label=\"Before Mutation\")\n",
    "after_mutation_plot, = axes[1, 0].plot(offspring_after_mutation, linestyle=\"-\", color=\"red\", label=\"After Mutation\")\n",
    "axes[1, 0].set_title(\"5. Mutation\")\n",
    "axes[1, 0].set_xlabel(\"Chromosome Index\")\n",
    "axes[1, 0].set_ylabel(\"Decision Variable\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 6. Replacement\n",
    "axes[1, 1].scatter(population[:, 0], population[:, 1], marker=\"o\", label=\"Before Replacement\")\n",
    "axes[1, 1].scatter(population_next_gen[:, 0], population_next_gen[:, 1], marker=\"x\", label=\"After Replacement\")\n",
    "axes[1, 1].set_title(\"6. Replacement\")\n",
    "axes[1, 1].set_xlabel(\"Variable 1\")\n",
    "axes[1, 1].set_ylabel(\"Variable 2\")\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# 7. Termination\n",
    "axes[1, 2].plot(best_fitness)\n",
    "axes[1, 2].set_title(\"7. Termination\")\n",
    "axes[1, 2].set_xlabel(\"Generation\")\n",
    "axes[1, 2].set_ylabel(\"Best Fitness Value\")\n",
    "\n",
    "# Remove the last subplot\n",
    "axes[1, 3].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **GA algorithm** is configured with these parameters:\n",
    "* pop_size=100: A population size of 100 individuals, which balances exploration of the solution space and computational cost.\n",
    "* n_offsprings=50: 50 offspring are generated in each generation, balancing convergence speed and computational cost.\n",
    "* sampling=FloatRandomSampling(): FloatRandomSampling is used for the initial population generation, which generates random floating-point numbers <a href=\"https://pymoo.org/operators/sampling.html\">[2]</a>.\n",
    "    * FloatRandomSampling creates the initial population by generating random floating-point numbers for each decision variable within their respective lower and upper bounds. This ensures that the search process starts from various regions in the solution space, allowing the algorithm to explore different combinations of decision variable values and potentially find better solutions.\n",
    "* crossover=SBX(prob=0.9, eta=20): Simulated Binary Crossover (SBX) is employed with a probability of 0.9 and an eta value of 20, controlling exploration versus exploitation during crossover.\n",
    "    * SBX operates on two parent solutions to generate two offspring solutions by combining their decision variables. The key idea is to create offspring that are statistically distributed around the parents, similar to the way single-point crossover works with binary strings <a href=\"https://content.wolfram.com/uploads/sites/13/2018/02/09-2-2.pdf\">[3]</a>  <a href=\"https://dl.acm.org/doi/10.1145/1276958.1277190\">[4]</a>  <a href=\"https://pymoo.org/operators/crossover.html?highlight=simulated%20binary%20crossover%20sbx\">[5]</a>.The SBX operator works as follows:\n",
    "        * Select two parent solutions from the current population.\n",
    "        * For each decision variable, generate a random number 'u' between 0 and 1.\n",
    "        * Calculate the blending factor '' using the random number 'u', a distribution index '' (eta), and the corresponding decision variables of the parents.\n",
    "        * Create two offspring solutions by combining the parent decision variables using the blending factor ''.\n",
    "    * The distribution index '' controls the exploration-exploitation trade-off during crossover. A larger '' value results in offspring that are closer to their parents (exploitation), while a smaller '' value generates offspring that are more spread out in the search space (exploration).\n",
    "* mutation=PolynomialMutation(prob=0.1, eta=20): Polynomial Mutation is used with a probability of 0.1 and an eta value of 20, controlling mutation step size and exploration of the search space.\n",
    "    * Polynomial Mutation operates on a single solution and modifies its decision variables <a href=\"https://dl.acm.org/doi/10.1145/1276958.1277190\">[4]</a> <a href=\" https://pymoo.org/operators/mutation.html?highlight=polynomialmutation\">[6]</a>. The mutation operator works as follows:\n",
    "      * Select a solution from the current population.\n",
    "      * For each decision variable in the solution, generate a random number 'u' between 0 and 1.\n",
    "      * Calculate the mutation scaling factor '' using the random number 'u', a distribution index '' (eta), and the variable's lower and upper bounds.\n",
    "      * Add the mutation scaling factor '' to the decision variable to create the mutated variable.\n",
    "The distribution index '' controls the mutation step size and exploration of the search space. A larger '' value results in smaller mutation steps, generating offspring closer to their parent (exploitation), while a smaller '' value generates offspring further away from their parent (exploration).\n",
    "* eliminate_duplicates=True: This setting ensures that duplicate solutions are removed, maintaining diversity in the population.\n",
    "\n",
    "Further:\n",
    "from pymoo.algorithms.soo.nonconvex.ga import GA\n",
    "from pymoo.problems import get_problem\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.core.problem import Problem\n",
    "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
    "from pymoo.operators.crossover.sbx import SBX\n",
    "from pymoo.operators.mutation.pm import PolynomialMutation\n",
    "The **CustomProblem** takes two arguments: rmse_normalized and param_values_normalized. It calls the parent class constructor (Problem) with the following specifications:\n",
    "\n",
    "* n_var=1: The problem has 1 decision variable, corresponding to the index of the parameter values array.\n",
    "* n_obj=1: There is a single objective, which is to minimize the normalized RMSE.\n",
    "* n_constr=0: There are no constraints in this problem.\n",
    "* xl=np.array([0]) and xu=np.array([1]): The lower and upper bounds for the decision variable are 0 and 1, respectively.\n",
    "* it also stores the input rmse_normalized and param_values_normalized as class attributes for later use in the evaluation function\n",
    "\n",
    "**_evaluate method** is responsible for evaluating the objective function. It takes the input decision variable x and calculates the corresponding objective value (normalized RMSE) based on the stored rmse_normalized data. \n",
    "* First, it converts the input x to indices by scaling it with the length of the rmse_normalized array and rounding the result. \n",
    "* Then, it assigns the objective value out[\"F\"] by indexing the rmse_normalized array with the calculated indices. \n",
    "* This process allows the optimization algorithm to search for the optimal parameter values that minimize the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.algorithms.soo.nonconvex.ga import GA\n",
    "from pymoo.problems import get_problem\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.core.problem import Problem\n",
    "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
    "from pymoo.operators.crossover.sbx import SBX\n",
    "from pymoo.operators.mutation.pm import PolynomialMutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Surface** anlaysis Parameter Opti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom problem class\n",
    "class CustomProblem(Problem):\n",
    "    def __init__(self, rmse_normalized, param_values_normalized):\n",
    "        # Call the parent class constructor with problem specifications\n",
    "        super().__init__(n_var=1,\n",
    "                         n_obj=1,\n",
    "                         n_constr=0,\n",
    "                         xl=np.array([0]),\n",
    "                         xu=np.array([1]))\n",
    "\n",
    "        # Store the normalized RMSE and parameter values as class attributes\n",
    "        self.rmse_normalized = rmse_normalized\n",
    "        self.param_values_normalized = param_values_normalized\n",
    "\n",
    "    # Define the evaluation function\n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        # Convert the input x to indices\n",
    "        idx = np.round(x * (len(self.rmse_normalized) - 1)).astype(int)\n",
    "\n",
    "        # Calculate the objective values for the given indices\n",
    "        out[\"F\"] = self.rmse_normalized[idx]\n",
    "\n",
    "# Load the data from the CSV files\n",
    "lhc_df = pd.read_csv(f'{TableData_path}/{output_csv}', header=0)\n",
    "sim_hf_df = pd.read_csv(f'{TableData_path}/SIM_HF_surface_{stor_id}.csv', header=0)\n",
    "\n",
    "# Calculate the average RMSE and HRRPUA values\n",
    "rmse_values = sim_hf_df.loc[:, sim_hf_df.columns.str.startswith(\"RMSE\")].values\n",
    "rmse_average = rmse_values.mean(axis=1)\n",
    "\n",
    "# Normalize the RMSE and HRRPUA values\n",
    "rmse_normalized = (rmse_average - rmse_average.min()) / (rmse_average.max() - rmse_average.min())\n",
    "\n",
    "# Get the unique parameters from the lhc_df\n",
    "unique_params = lhc_df.columns.unique()[1:]\n",
    "\n",
    "# Initialize lists to store results and best values\n",
    "results_list = []\n",
    "best_values_list = []\n",
    "\n",
    "# Loop through each parameter and optimize using GA\n",
    "for param in unique_params:\n",
    "    # Get the parameter values and normalize them\n",
    "    param_values = lhc_df.loc[:, lhc_df.columns.str.startswith(param)].values.ravel()\n",
    "    param_values_normalized = (param_values - param_values.min()) / (param_values.max() - param_values.min())\n",
    "\n",
    "    # Create a custom problem instance for the current parameter\n",
    "    problem = CustomProblem(rmse_normalized, param_values_normalized)\n",
    "\n",
    "    # Define the GA algorithm with the specified parameters\n",
    "    algorithm = GA(\n",
    "        pop_size=100,\n",
    "        n_offsprings=50,\n",
    "        sampling=FloatRandomSampling(),\n",
    "        crossover=SBX(prob=0.9, eta=20),\n",
    "        mutation=PolynomialMutation(prob=0.1, eta=20),\n",
    "        eliminate_duplicates=True,\n",
    "    )\n",
    "\n",
    "    # Perform the optimization\n",
    "    res = minimize(\n",
    "        problem,\n",
    "        algorithm,\n",
    "        (\"n_gen\", 50),\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    pareto_front = res.F\n",
    "    best_solution = res.X[np.argmin(res.F)]\n",
    "    best_value = param_values[int(np.round(best_solution * (len(param_values) - 1)))]\n",
    "    ###Add x % to min and max \n",
    "    best_value_minus_10 = best_value - 0.01 * best_value\n",
    "    best_value_plus_10 = best_value + 0.01 * best_value\n",
    "    ###Clip if default is reached\n",
    "    lower_bound, upper_bound = default_ranges[param]\n",
    "    clipped_lower_bound = np.clip(best_value_minus_10, lower_bound, upper_bound)\n",
    "    clipped_upper_bound = np.clip(best_value_plus_10, lower_bound, upper_bound)\n",
    "\n",
    "    best_values_list.append((param, clipped_lower_bound, clipped_upper_bound))\n",
    "\n",
    "output1=[]\n",
    "# Print the best values and their clipped ranges\n",
    "print(\"\\nBest values and their clipped ranges:\")\n",
    "for best_value_info in best_values_list:\n",
    "    print(best_value_info)\n",
    "    output1.append(best_value_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Time** Analysis Parameter Opti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.algorithms.soo.nonconvex.ga import GA\n",
    "from pymoo.problems import get_problem\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.core.problem import Problem\n",
    "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
    "from pymoo.operators.crossover.sbx import SBX\n",
    "from pymoo.operators.mutation.pm import PolynomialMutation\n",
    "\n",
    "# Define the custom problem class\n",
    "class CustomProblem(Problem):\n",
    "    def __init__(self, rmse_normalized, param_values_normalized):\n",
    "        # Call the parent class constructor with problem specifications\n",
    "        super().__init__(n_var=1,\n",
    "                         n_obj=1,\n",
    "                         n_constr=0,\n",
    "                         xl=np.array([0]),\n",
    "                         xu=np.array([1]))\n",
    "\n",
    "        # Store the normalized RMSE and parameter values as class attributes\n",
    "        self.rmse_normalized = rmse_normalized\n",
    "        self.param_values_normalized = param_values_normalized\n",
    "\n",
    "    # Define the evaluation function\n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        # Convert the input x to indices\n",
    "        idx = np.round(x * (len(self.rmse_normalized) - 1)).astype(int)\n",
    "\n",
    "        # Calculate the objective values for the given indices\n",
    "        out[\"F\"] = self.rmse_normalized[idx]\n",
    "\n",
    "# Load the data from the CSV files\n",
    "lhc_df = pd.read_csv(f'{TableData_path}/{output_csv}', header=0)\n",
    "sim_hf_df = pd.read_csv(f'{TableData_path}/SIM_avg_HF_Center_{stor_id}.csv', header=0)\n",
    "\n",
    "# Calculate the average RMSE and HRRPUA values\n",
    "rmse_values = sim_hf_df.loc[:, sim_hf_df.columns.str.startswith(\"RMSE\")].values\n",
    "rmse_average = rmse_values.mean(axis=1)\n",
    "\n",
    "# Normalize the RMSE and HRRPUA values\n",
    "rmse_normalized = (rmse_average - rmse_average.min()) / (rmse_average.max() - rmse_average.min())\n",
    "\n",
    "# Get the unique parameters from the lhc_df\n",
    "unique_params = lhc_df.columns.unique()[1:]\n",
    "\n",
    "# Initialize lists to store results and best values\n",
    "results_list = []\n",
    "best_values_list = []\n",
    "\n",
    "# Loop through each parameter and optimize using GA\n",
    "for param in unique_params:\n",
    "    # Get the parameter values and normalize them\n",
    "    param_values = lhc_df.loc[:, lhc_df.columns.str.startswith(param)].values.ravel()\n",
    "    param_values_normalized = (param_values - param_values.min()) / (param_values.max() - param_values.min())\n",
    "\n",
    "    # Create a custom problem instance for the current parameter\n",
    "    problem = CustomProblem(rmse_normalized, param_values_normalized)\n",
    "\n",
    "    # Define the GA algorithm with the specified parameters\n",
    "    algorithm = GA(\n",
    "        pop_size=100,\n",
    "        n_offsprings=50,\n",
    "        sampling=FloatRandomSampling(),\n",
    "        crossover=SBX(prob=0.9, eta=20),\n",
    "        mutation=PolynomialMutation(prob=0.1, eta=20),\n",
    "        eliminate_duplicates=True,\n",
    "    )\n",
    "\n",
    "    # Perform the optimization \n",
    "    res = minimize(\n",
    "        problem,\n",
    "        algorithm,\n",
    "        (\"n_gen\", 50),\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    pareto_front = res.F\n",
    "    best_solution = res.X[np.argmin(res.F)]\n",
    "    best_value = param_values[int(np.round(best_solution * (len(param_values) - 1)))]\n",
    "    ###Add x % to min and max \n",
    "    best_value_minus_10 = best_value - 0.01 * best_value\n",
    "    best_value_plus_10 = best_value + 0.01 * best_value\n",
    "    ###Clip if default is reached\n",
    "    lower_bound, upper_bound = default_ranges[param]\n",
    "    clipped_lower_bound = np.clip(best_value_minus_10, lower_bound, upper_bound)\n",
    "    clipped_upper_bound = np.clip(best_value_plus_10, lower_bound, upper_bound)\n",
    "\n",
    "    best_values_list.append((param, clipped_lower_bound, clipped_upper_bound))\n",
    "\n",
    "output2=[]\n",
    "# Print the best values and their clipped ranges\n",
    "print(\"\\nBest values and their clipped ranges:\")\n",
    "for best_value_info in best_values_list:\n",
    "    print(best_value_info)\n",
    "    output2.append(best_value_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the two outputs to creat new Ranges for the next LHC Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output can be copied to LHS_FDS...ipynb in params_info cell\n",
    "combined_ranges = []\n",
    "\n",
    "for param1, param2 in zip(output1, output2):\n",
    "    param_name, min1, max1 = param1\n",
    "    _, min2, max2 = param2\n",
    "\n",
    "    combined_min = min(min1, min2)\n",
    "    combined_max = max(max1, max2)\n",
    "\n",
    "    # Add the additional \"values\" for each parameter\n",
    "    if param_name == 'PATH_LENGTH':\n",
    "        lhs_method = 'simple'\n",
    "        param_type = None\n",
    "    elif param_name == 'ANGLE_INCREMENT':\n",
    "        lhs_method = 'simple'\n",
    "        param_type = None\n",
    "    elif param_name == 'NUMBER_RADIATION_ANGLES':\n",
    "        lhs_method = 'LHS'\n",
    "        param_type = None\n",
    "    elif param_name == 'HRRPUA':\n",
    "        lhs_method = 'LHS'\n",
    "        param_type = None\n",
    "    elif param_name == 'SOOT_YIELD':\n",
    "        lhs_method = 'LHS'\n",
    "        param_type = None\n",
    "    elif param_name == 'RADIATIVE_FRACTION':\n",
    "        lhs_method = 'LHS'\n",
    "        param_type = None\n",
    "    elif param_name == 'TMPA':\n",
    "        lhs_method = 'LHS'\n",
    "        param_type = None\n",
    "    elif param_name == 'EMISSIVITY_Burner':\n",
    "        lhs_method = 'LHS'\n",
    "        param_type = 'random'\n",
    "    elif param_name == 'EMISSIVITY_Panel':\n",
    "        lhs_method = 'LHS'\n",
    "        param_type = 'random'\n",
    "    else:\n",
    "        lhs_method = None\n",
    "        param_type = None\n",
    "\n",
    "    combined_ranges.append((param_name, combined_min, combined_max, lhs_method,param_type))\n",
    "\n",
    "for param_range in combined_ranges:\n",
    "    print(f'{param_range},')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History of Parameter Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the directory containing the text files\n",
    "path_over = \"../BurnerSims/\"\n",
    "\n",
    "# Define a dictionary to store the parameter ranges for each text file\n",
    "param_ranges_txt = {}\n",
    "\n",
    "# Loop through each text file\n",
    "for folder in os.listdir(path_over):\n",
    "    if folder.startswith(label_session[:-14]):\n",
    "        folder_path = os.path.join(path_over, folder)\n",
    "        for data in os.listdir(folder_path):\n",
    "            if data.startswith(\"param_set_ranges_\"):\n",
    "                text_path = os.path.join(folder_path, data)\n",
    "\n",
    "                # Open the file for reading\n",
    "                with open(text_path, 'r') as file:\n",
    "                    file.readline()\n",
    "\n",
    "                    for line in file:\n",
    "                        param_name, value1, value2, value3, value4 = [s.strip() for s in re.split(r':|,', line)]\n",
    "\n",
    "                        # Convert the values from string to float\n",
    "                        value1 = float(value1)\n",
    "                        value2 = float(value2)\n",
    "\n",
    "                        # Add the parameter range \n",
    "                        if param_name not in param_ranges_txt:\n",
    "                            param_ranges_txt[param_name] = []\n",
    "                        param_ranges_txt[param_name].append((value1, value2))\n",
    "\n",
    "# DataFrame to store the parameter ranges \n",
    "pd.DataFrame(param_ranges_txt).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
